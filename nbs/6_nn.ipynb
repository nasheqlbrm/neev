{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff2d3be",
   "metadata": {},
   "source": [
    "# nn\n",
    "\n",
    "> A description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d616e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42140ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3744ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import random\n",
    "from neev.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da004a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, \n",
    "                 nin, # number of inputs to the neuron\n",
    "                 nonlin=True # do we have a non-linearity at the end\n",
    "                ):\n",
    "        self.w = [Value(random.uniform(-1,1)) for i in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        self.nonlin = nonlin\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        act =  sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"   \n",
    "    \n",
    "class Layer(Module):\n",
    "    def __init__(self, \n",
    "                 nin,#number of inputs to each neuron in the layer \n",
    "                 nout,#number of neurons in the layer\n",
    "                 **kwargs\n",
    "                ):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"    \n",
    "\n",
    "    \n",
    "class MLP(Module):\n",
    "    def __init__(self, \n",
    "                 nin,#number of inputs to each neuron in the layer  \n",
    "                 nouts # list with the number of neurons in each layer of the MLP\n",
    "                ):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d09cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.5878484569575098, grad=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "x = [2.0,3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f2bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9133910750788039, grad=0),\n",
       " Value(data=0, grad=0),\n",
       " Value(data=0, grad=0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "# we want 3 neurons in our layer, each neuron will take two inputs\n",
    "l = Layer(2,3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.3513214887908616, grad=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### |hide\n",
    "# we want 3 layers in our MLP\n",
    "# with 4 neurons in the first, 4 in the second\n",
    "# 1 neuron in the output layer\n",
    "# each neuron in the MLP will take three inputs\n",
    "n = MLP(3,[4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert len(n.parameters()) == 4*4 + 4*5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c167b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "ys= [1.0,-1.0,-1.0,1.0] #targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920b849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,2.7333176008876476\n",
      "1,1.8921703655494224\n",
      "2,1.2660917509071825\n",
      "3,0.9472041421564896\n",
      "4,0.9206885620508152\n",
      "5,0.975091868857794\n",
      "6,0.8926058860157713\n",
      "7,0.5597824418937001\n",
      "8,0.47472393385540357\n",
      "9,0.7111006727436218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.07924014239999504, grad=-0.4603799288000025),\n",
       " Value(data=-1.648413092033964, grad=-0.32420654601698196),\n",
       " Value(data=-1.7810042524501783, grad=-0.3905021262250892),\n",
       " Value(data=0.017046900742157672, grad=-0.4914765496289212)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "#     print(ypred)\n",
    "    loss = sum((yout-ygt)**2 for ygt,yout in zip(ys,ypred))/len(ys)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "#     print(n.layers[0].neurons[0].w[0].grad)\n",
    "#     print(n.layers[0].neurons[0].w[0].data)\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 *p.grad\n",
    "        \n",
    "    print(f'{k},{loss.data}')\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9fda42",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "The training loop above has a bug. Can you spot it?\n",
    "\n",
    "Essentially we forgot to zero out the gradients!! The gradients continued to accumulate and essentially gave us a huge step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "n = MLP(3,[4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11235390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,2.8170895040170354\n",
      "1,0.9969270672890835\n",
      "2,0.7533475867144054\n",
      "3,0.61082591721944\n",
      "4,0.5156173863611934\n",
      "5,0.44874168278198845\n",
      "6,0.4006391992486378\n",
      "7,0.3653970997569263\n",
      "8,0.3429110035584173\n",
      "9,0.32362653412514897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=1.013470917055236, grad=0.006735458527618032),\n",
       " Value(data=-0.8237837837082684, grad=0.08810810814586578),\n",
       " Value(data=0.05145731432323194, grad=0.525728657161616),\n",
       " Value(data=0.6028727758434476, grad=-0.19856361207827622)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "    loss = sum((yout-ygt)**2 for ygt,yout in zip(ys,ypred))/len(ys)\n",
    "    \n",
    "    # backward pass\n",
    "    n.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "        \n",
    "    print(f'{k},{loss.data}')\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ad3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
