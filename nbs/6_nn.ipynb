{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38077b3d",
   "metadata": {},
   "source": [
    "# nn\n",
    "\n",
    "> A description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c56c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12df5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import random\n",
    "from neev.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c31359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, \n",
    "                 nin, # number of inputs to the neuron\n",
    "                 nonlin=True # do we have a non-linearity at the end\n",
    "                ):\n",
    "        self.w = [Value(random.uniform(-1,1)) for i in range(nin)]\n",
    "        self.b = Value(0.)\n",
    "        self.nonlin = nonlin\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        act =  sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"   \n",
    "    \n",
    "class Layer(Module):\n",
    "    def __init__(self, \n",
    "                 nin,#number of inputs to each neuron in the layer \n",
    "                 nout,#number of neurons in the layer\n",
    "                 **kwargs\n",
    "                ):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"    \n",
    "\n",
    "    \n",
    "class MLP(Module):\n",
    "    def __init__(self, \n",
    "                 nin,#number of inputs to each neuron in the layer  \n",
    "                 nouts # list with the number of neurons in each layer of the MLP\n",
    "                ):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc9108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "x = [2.0,3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d1970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.10968307409657507),\n",
       " Value(data=1.2650493394385491),\n",
       " Value(data=0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "# we want 3 neurons in our layer, each neuron will take two inputs\n",
    "l = Layer(2,3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cc3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9172487555710265)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### |hide\n",
    "# we want 3 layers in our MLP\n",
    "# with 4 neurons in the first, 4 in the second\n",
    "# 1 neuron in the output layer\n",
    "# each neuron in the MLP will take three inputs\n",
    "n = MLP(3,[4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert len(n.parameters()) == 4*4 + 4*5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ddebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from neev.viz import view_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfee516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# view_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "ys= [1.0,-1.0,-1.0,1.0] #targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,2.2507702656896567\n",
      "1,1.0573550260509528\n",
      "2,0.9285578490636487\n",
      "3,1.0\n",
      "4,1.0\n",
      "5,1.0\n",
      "6,1.0\n",
      "7,1.0\n",
      "8,1.0\n",
      "9,1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0), Value(data=0), Value(data=0), Value(data=0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "#     print(ypred)\n",
    "    loss = sum((yout-ygt)**2 for ygt,yout in zip(ys,ypred))/len(ys)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "#     print(n.layers[0].neurons[0].w[0].grad)\n",
    "#     print(n.layers[0].neurons[0].w[0].data)\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 *p.grad\n",
    "        \n",
    "    print(f'{k},{loss.data}')\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c0c32",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "The training loop above has a bug. Can you spot it?\n",
    "\n",
    "Essentially we forgot to zero out the gradients!! The gradients continued to accumulate and essentially gave us a huge step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "n = MLP(3,[4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdd1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1.6644128972764667\n",
      "1,0.7255956439240124\n",
      "2,0.6455231759395288\n",
      "3,0.5820697987413569\n",
      "4,0.5697417379478957\n",
      "5,0.5466548811467331\n",
      "6,0.5453084951617699\n",
      "7,0.5396681405739372\n",
      "8,0.5361309242654972\n",
      "9,0.5330331309996946\n",
      "10,0.5301898481687445\n",
      "11,0.5275682519116583\n",
      "12,0.5265336352943062\n",
      "13,0.5367907544633079\n",
      "14,0.5270148361736797\n",
      "15,0.5239376733113821\n",
      "16,0.5217004750491708\n",
      "17,0.5196930446614094\n",
      "18,0.5178537781584924\n",
      "19,0.5161674107820329\n",
      "20,0.5146228677047072\n",
      "21,0.5132099979015661\n",
      "22,0.5119192855472541\n",
      "23,0.5107417839323185\n",
      "24,0.5096690725695857\n",
      "25,0.5086932231376619\n",
      "26,0.5078067719731929\n",
      "27,0.5070026975213052\n",
      "28,0.5062744013666103\n",
      "29,0.5056156917099882\n",
      "30,0.5050207683942108\n",
      "31,0.5044842087944673\n",
      "32,0.5040009540735206\n",
      "33,0.5035662954559021\n",
      "34,0.503175860302974\n",
      "35,0.5028255978731135\n",
      "36,0.5025117647311684\n",
      "37,0.5022309098313266\n",
      "38,0.5019798593403136\n",
      "39,0.5017557012959952\n",
      "40,0.5015557702125996\n",
      "41,0.5013776317502308\n",
      "42,0.5012190675653134\n",
      "43,0.5010780604520058\n",
      "44,0.5009527798741213\n",
      "45,0.5008415679741094\n",
      "46,0.5015479861163684\n",
      "47,0.5107908546343253\n",
      "48,0.502400465858282\n",
      "49,0.5017374823947409\n",
      "50,0.501517303068389\n",
      "51,0.5013371718346544\n",
      "52,0.5011780984128512\n",
      "53,0.501037276963711\n",
      "54,0.5009127132609337\n",
      "55,0.5008026280347407\n",
      "56,0.5007054207389646\n",
      "57,0.500619653859103\n",
      "58,0.5005440386216037\n",
      "59,0.5004774215706789\n",
      "60,0.5004187720263141\n",
      "61,0.5003671704246346\n",
      "62,0.5003217975271719\n",
      "63,0.5002819244755172\n",
      "64,0.5002469036602841\n",
      "65,0.5002161603676127\n",
      "66,0.5001891851623784\n",
      "67,0.5001655269646014\n",
      "68,0.5001447867740821\n",
      "69,0.5001266119978125\n",
      "70,0.500110691335066\n",
      "71,0.5000967501760637\n",
      "72,0.5000845464716163\n",
      "73,0.5000738670330208\n",
      "74,0.5000645242236289\n",
      "75,0.500056353005813\n",
      "76,0.5000492083094624\n",
      "77,0.5000429626905755\n",
      "78,0.5000375042509266\n",
      "79,0.5000327347921458\n",
      "80,0.5000285681798229\n",
      "81,0.5000249288954094\n",
      "82,0.500021750755737\n",
      "83,0.5000189757818864\n",
      "84,0.5000165532009205\n",
      "85,0.5000144385656452\n",
      "86,0.5000125929790726\n",
      "87,0.5000109824116533\n",
      "88,0.5000095771006041\n",
      "89,0.5000083510218126\n",
      "90,0.5000072814258312\n",
      "91,0.5000063484304206\n",
      "92,0.5000055346629427\n",
      "93,0.5000048249466588\n",
      "94,0.5000042060256732\n",
      "95,0.5000036663238677\n",
      "96,0.5000031957337093\n",
      "97,0.5000027854313014\n",
      "98,0.5000024277144746\n",
      "99,0.5000021158610938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=1.0013313465105345),\n",
       " Value(data=0),\n",
       " Value(data=0),\n",
       " Value(data=0.9974133108335308)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "    loss = sum((yout-ygt)**2 for ygt,yout in zip(ys,ypred))/len(ys)\n",
    "    \n",
    "    # backward pass\n",
    "    n.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "        \n",
    "    print(f'{k},{loss.data}')\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471413f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# view_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dd8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
