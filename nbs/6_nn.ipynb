{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1be241",
   "metadata": {},
   "source": [
    "# nn\n",
    "\n",
    "> A description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040947d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import random\n",
    "from neev.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Neuron:\n",
    "    def __init__(self, \n",
    "                 nin # number of inputs to the neuron\n",
    "                ):\n",
    "        self.w = [Value(random.uniform(-1,1)) for i in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        act =  sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        o = act.tanh()\n",
    "        return o\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, \n",
    "                 nin,#number of inputs to each neuron in the layer \n",
    "                 nout#number of neurons in the layer\n",
    "                ):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, \n",
    "                 nin,#number of inputs to each neuron in the layer  \n",
    "                 nouts # list with the number of neurons in each layer of the MLP\n",
    "                ):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60001d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.995302467957026)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "x = [2.0,3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf8cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9723065449371219),\n",
       " Value(data=0.8964959769112831),\n",
       " Value(data=-0.06784727993965199)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "# we want 3 neurons in our layer, each neuron will take two inputs\n",
    "l = Layer(2,3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b489bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.6530463189343721)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### |hide\n",
    "# we want 3 layers in our MLP\n",
    "# with 4 neurons in the first, 4 in the second\n",
    "# 1 neuron in the output layer\n",
    "# each neuron in the MLP will take three inputs\n",
    "n = MLP(3,[4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert len(n.parameters()) == 4*4 + 4*5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df17f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from neev.viz import view_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# view_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "ys= [1.0,-1.0,-1.0,1.0] #targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "#     print(ypred)\n",
    "    loss = sum((yout-ygt)**2 for ygt,yout in zip(ys,ypred))\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "#     print(n.layers[0].neurons[0].w[0].grad)\n",
    "#     print(n.layers[0].neurons[0].w[0].data)\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 *p.grad\n",
    "        \n",
    "    print(f'{k},{loss.data}')\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c67879",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "The training loop above has a bug. Can you spot it?\n",
    "\n",
    "Essentially we forgot to zero out the gradients!! The gradients continued to accumulate and essentially gave us a huge step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "n = MLP(3,[4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138db9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "    loss = sum((yout-ygt)**2 for ygt,yout in zip(ys,ypred))\n",
    "    \n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0 #zero grad\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "        p.grad = 0\n",
    "        \n",
    "    print(f'{k},{loss.data}')\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# view_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
